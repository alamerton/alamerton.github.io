---
layout: post
title: "A Technical Implementation of a Common Vague Goal"
---
A common goal of the current global policy actions is encouraging the ethical use of AI. One way that could be implemented is by creating an organisation to whom risky AI developments get reported.

![Diagram showing how the organisation proposed might look]('assets/diagram_1.png')

There are suggestions to AI auditing companies, but none of these have suggested creating an intermediary audit.
## Intro

There are multiple avenues to unrecoverable bad futures resulting from AI systems. In this document, processes that cause unrecoverable bad futures resulting from AI are referred to as AI-enabled lock-in risks. The development of these risks can be categorised by their level of progress. For example, one categorisation might look like this:

![Diagram showing how the development of risks might look]('assets/risks.png')

Categorising risks can enable individuals who may be involved with organisations prone to such risks to learn how to identify and report on incidents where they believe a risk may be developing. In this document I propose an organisation which gives individuals and organisations a place to report on potential AI-enabled lock-in risk developments.

## What Problem Is It Solving?

**What it helps with**
If there exists an organisation which enables the reporting of these risk developments, it may reduce the chance of humanity ending up in an unrecoverable bad future.
But what would the organisation do if it identifies a risk?

**Why it is important**
1. Unrecoverable bad futures are one of the main risks posed by advanced AI systems
2. By their nature, lock-in risks prevent us from being able to easily reverse them
3. Humanity could be at risk of losing aspects of the future we desire such as happiness or individual freedom
4. There does not currently exist a system for reporting on AI-enabled lock-in risks, so the chance of risks developing without intervention is higher than if an effective version of such a system existed

**What it doesn’t directly help with**
There is not a direct mitigation being proposed here, but a framework for identifying and reporting on risks. Employing mitigation strategies would have more of an impact on reducing lock-in risks [I hope to be able to fill this part in more].

## How it Solves the Problem
**Identifying developmental stages of lock-in risk**
Risk reporting protocols would help individuals flag concerning developments, especially if developments could be categorised. [TODO: Research other risk definition strategies] One approach to categorising the development of a lock-in risk is by their level of progress. A well-defined set of developmental stages of AI-enabled lock-in risks can then be broadcast to organisations prone to instantiating or propagating such risks. The more coherent and concrete the stages, the more digestible they might be to individuals.

Identifying these stages could help identify correlations in avenues to risk. If every developmental stage from no risk to instantiated risk was identified for every unrecoverable bad future, we could extract the most commonly occurring risk profiles and target interventions at them. We may learn if there are underlying themes that characterise lock-in risk development, and design concrete strategies which target these themes.

**Developing risk reporting protocols for each stage**
Protocols can then be designed for individuals to report on identified potential risks. These protocols should be designed with over communication, ease of reporting, reporting incentives, transparency, and accountability dynamics in mind. The best reporting protocol is one which is either decentralised, or centrally controlled by a trusted organisation, and that results in the highest number of potential risks being reported (within reason).

What accountability dynamics might characterise successful reporting protocols? How can individuals be given autonomy and power in the reporting framework? Would distributing power lead to a lower likelihood of forms of lock-in taking effect on the framework itself? 

**Building an organisation which takes action on risk reports**
Some organisation of individuals should be formed to ensure that the risk reporting protocols result in impactful risk-mitigation outcomes. The duty of the organisation would be to take relevant action on risks when they are reported. Protocols could be designed for taking action on reported risks, or reports could be responded to on a case-by-case basis, depending on what seems most helpful. This organisation might be connected to bodies which can enforce regulation on other organisations, or those which have the authority to conduct audits on organisations. 

How would the performance/impact of this organisation be measured? 

**Ways it could fail to help solve the problem**
It might be the case that it is too hard to find a useful categorisation of the developmental stages of lock-in risks. This would not necessarily mean that the system would not work, but that the protocols may have to be designed differently.

It is also possible that individuals and organisations ignore or refuse to adopt the system. They may not want to share sensitive information about their organisation or have conflicting interests preventing them from reporting against their own organisation.
## Conclusion
This post creates the foundation for a system which could have an impact on understanding and mitigating against AI-enable lock-in risks. It does so by breaking down the development of risks into stages and proposing the design of reporting protocols and intervention strategies which could be managed by a new organisation. The next steps in validating the promise of such a system are a closer examination of the failure modes of the system, looking for parallel processes in other domains concerning risk reporting and mitigation, and developing a first iteration of a developmental categorisation of lock-in risks.
